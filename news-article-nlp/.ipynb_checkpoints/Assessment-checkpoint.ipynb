{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echobox News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages (spacy, sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import spacy\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "articles = pd.read_csv(\"20190710 - DS Challenge - Breaking News Articles Detection - DSChallengeArticleId.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates\n",
    "articles[articles.duplicated()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert theunix timestamp to datetime item in python\n",
    "articles['ArticlePublishedTimeDateTime'] = articles['ArticlePublishedTime'].apply(lambda x: datetime.fromtimestamp(int(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the empty article descriptions with an empty string to avoid errors\n",
    "articles['ArticleDescription'].fillna(\"\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the range published times of articles\n",
    "max(articles['ArticlePublishedTimeDateTime'])-min(articles['ArticlePublishedTimeDateTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Challenges\n",
    "There are a couple of interesting parameters surrounding the dataset\n",
    "1. There are only 13,000 articles in the dataset\n",
    "    - This makes it difficult to train a dictionary with word2vec model as there is insufficient data to form a sufficiently coherent dictionary\n",
    "2. News articles have a wide variety of topics from sports to politics to celebrity news\n",
    "    - NLP models are usually specific to a dataset as it is difficult to generate/train a model that is generic enough that can be applied to all texts\n",
    "    - The wide variety of topics makes it difficult for any sort of dictinoary not trained specifically on this dataset to perform well\n",
    "3. Not all articles have descriptions\n",
    "    - My initial thought was to have a hierachical method of clustering using both the article titles and descriptions. With only slightly over half of the articles having descriptions this is not a viable solution\n",
    "4. The dataset is completely unsupervised\n",
    "    - This aspect makes it difficult to discern between articles that report major events.\n",
    "    - Without labelled data it is not trivial to train a model that is able to differentiate the major articles from the rest.\n",
    "    - From an unsupervised point of view, what is considered a 'major' article? Without labelled data we will have to work based off the features of the article vectors alone which is difficult to understand and represent\n",
    "5. The range of dates for the article is only roughly 2 days\n",
    "    - I was also considering clustering articles based on time of publish but the timeframe is very short. Will need to explore to determine if it is a good method of splitting the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Based On Vector Embeddings\n",
    "\n",
    "1. Apply preprocessing get vector representation of articles\n",
    "2. Perform dimensionality reduction to reduce the vectors from 300 features to 2 using PCA and TSNE\n",
    "3. Perform clusterisation using DBSCAN algorithm\n",
    "4. Evaluate the clusterisation using similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preprocessing and Applying Pre-Trained Model\n",
    "Preprocessing steps involve removing stop words and applying a pretrained 'spacy' model to get the vector embeddings for the articles. The pre-trained model I will be using was trained on Wikipedia articles and has over 1,000,000 different words. Although not perfect, it should at least be able to represent each word of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "sent_vec={}\n",
    "docs = []\n",
    "type_dict = {}\n",
    "# sentences = {}\n",
    "# sentences['sentence'] = []\n",
    "preprocessed_articles = []\n",
    "for article in articles.ArticleTitle:\n",
    "#     words = [porter.stem(word) for word in word_tokenize(article) if word not in stop_words and word.isalpha()]\n",
    "#     sentences['sentence'].append(words)\n",
    "    doc = nlp(article)\n",
    "#     print([(X, X.ent_iob_, X.ent_type_) for X in doc])\n",
    "#     print([x for x in doc if x.ent_type_ == 'LOC' or x.ent_type_ == 'GPE'])\n",
    "    tokens = [token.text for token in doc if not token.is_stop and token.text.isalpha()]\n",
    "    new_article = ' '.join(tokens)\n",
    "    preprocessed_articles.append(new_article)\n",
    "#     print(new_article)\n",
    "    doc = nlp(new_article)\n",
    "    \n",
    "    for x in doc:\n",
    "        type_dict[x.ent_type_] = type_dict.get(x.ent_type_,0) + 1\n",
    "             \n",
    "    docs.append(doc)\n",
    "    sent_vec.update({article:doc.vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(type_dict.items(), key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(sent_vec.keys())\n",
    "vectors = list(sent_vec.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first perform PCA to reduce the dimensions to a reasonable number before using TSNE. TSNE is not suitable for reducing very high dimensional data so PCA will be used first as it is more mathematically stable and retains the variance of the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to reduce from 300 to 150\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "pca_150 = PCA(n_components=150)\n",
    "pca_result_150 = pca_150.fit_transform(vectors)\n",
    "\n",
    "print('Cumulative explained variation for 150 principal components: {}'.format(np.sum(pca_150.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "87% of the original variance is retained within the first 150 principal components. A plot of the cumulative variances for all possible number of components is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trend = []\n",
    "# for i in range(300):\n",
    "pca_model = PCA(n_components = 299)\n",
    "pca_results = pca_model.fit_transform(vectors)\n",
    "# trend.append(np.cumsum(pca_model.explained_variance_ratio_))\n",
    "\n",
    "sns.lineplot(data = np.cumsum(pca_model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150 principal components chosen as it retains the ~90% of the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE is then used to reduce further to 2 dimensions. Perplexity of 120 is chosen after various trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=300, random_state=7)\n",
    "tsne_results = tsne.fit_transform(pca_result_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2d vector representations to check for formation of clusters and patterns\n",
    "\n",
    "tsne_one = tsne_results[:,0]\n",
    "tsne_two = tsne_results[:,1]\n",
    "\n",
    "sns.scatterplot(x = tsne_one, y = tsne_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Clusterisation\n",
    "DBSCAN algorithm used as k does not need to be specified. Moreover, it is able to discern articles that do not belong to any cluster. However, epsilon parameter that determines the size of the cluster needs to be fine-tuned. This is done by trying for various epsilons within a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try DBSCAN clusterising algorithm for various values of epsilon\n",
    "n_classes = {}\n",
    "\n",
    "for i in np.arange(0.001,0.2,0.0002):\n",
    "    print(i)\n",
    "    dbscan = DBSCAN(metric='euclidean',eps=i, min_samples=2).fit(np.array(np.array(tsne_results)))\n",
    "    n_classes[i] = len(set(dbscan.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graph to identify best epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=list(n_classes.keys()), y = list(n_classes.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform DBSCAN once again and compile results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(metric='euclidean',eps=0.0518, min_samples=2).fit(np.array(np.array(tsne_results)))\n",
    "# dbscan = DBSCAN(metric='euclidean',eps=0.03, min_samples=2).fit(np.array(np.array(tsne_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results into a dataframe\n",
    "results = pd.DataFrame({'sentences':sentences,'preprocessed':preprocessed_articles, 'vectors':vectors, 'red-vectors':reduced_vec, 'descriptions':articles['ArticleDescription'], 'labels':dbscan.labels_, 'published':articles['ArticlePublishedTimeDateTime']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reduced vectors \n",
    "reduced_vec = [np.array(vec) for vec in tsne_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results[results['labels'] == 22].sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp(list(results[results['labels'] == 20].sentences)[0]).similarity(nlp(list(results[results['labels'] == 20].sentences)[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Evaluation\n",
    "As there is no established method of evaluating our final clusters as the dataset is unsupervised, I have defined a function that outputs the average semantic similarities for articles that have been clustered together. A value of 1 meaning the text has very high similarity and a value of 0 meaning they have nothing in common at all from a text point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(articles):\n",
    "    nlp_articles = [nlp(article) for article in articles]\n",
    "    similarities = []\n",
    "    for i in range(len(nlp_articles)-1):\n",
    "        for j in range(i,len(nlp_articles)):\n",
    "            similarities.append(nlp_articles[i].similarity(nlp_articles[j]))\n",
    "    return sum(similarities)/len(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to evaluate all the clusters\n",
    "evaluation = pd.DataFrame(results.groupby(['labels'])['preprocessed'].apply(find_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ratio of clusters accurately clustered:', evaluation[evaluation['preprocessed']>0.85].count()/(evaluation.shape[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, only 20% of the clusters can be considered to contain articles that are at least 85% similar to each other. This indicates the method of relying on the vector representations of the articles alone with DBSCAN is not sufficient in properly clustering these articles.\n",
    "\n",
    "Maybe I should consider grouping by time first before applying the clustering algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(results[results['labels'].isin(evaluation[evaluation['preprocessed']>0.85].index)].groupby('labels')['published'].apply(lambda x:max(x)-min(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations show that the average time between articles for the accurate clusters is 13 hours. Since the total range is 2 days, it is not beneficial to use time to pre-cluster the articles.\n",
    "\n",
    "Considering that a pure vector embedding clustering algorithm only reveals subpar results, maybe a more manual form of clustering first should be attempted ie. according to named entity recognition. This would also help in extracting information about the news articles as part of the second challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation)\n",
    "print(list(results[results['labels'] == 2450].sentences))\n",
    "print(list(results[results['labels'] == 2450].published))\n",
    "# print(list(results[results['labels'] == 2220].sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec={}\n",
    "docs = []\n",
    "type_dict = {}\n",
    "# sentences = {}\n",
    "# sentences['sentence'] = []\n",
    "preprocessed_articles = []\n",
    "for article in articles.ArticleTitle:\n",
    "#     words = [porter.stem(word) for word in word_tokenize(article) if word not in stop_words and word.isalpha()]\n",
    "#     sentences['sentence'].append(words)\n",
    "    doc = nlp(article)\n",
    "#     print([(X, X.ent_iob_, X.ent_type_) for X in doc])\n",
    "#     print([x for x in doc if x.ent_type_ == 'LOC' or x.ent_type_ == 'GPE'])\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and token.text.isalpha()]\n",
    "    new_article = ' '.join(tokens)\n",
    "    preprocessed_articles.append(new_article)\n",
    "#     print(new_article)\n",
    "    doc = nlp(new_article)\n",
    "    ner = []\n",
    "    types = ['PERSON', 'ORG', 'NORP', 'LOC', 'GPE']\n",
    "    tmp = None\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].ent_type_ in types:\n",
    "            tmp = doc[i].text\n",
    "            prev_index = i\n",
    "            next_index = i+1\n",
    "            while next_index < len(doc) and doc[next_index].ent_iob_ != doc[prev_index].ent_iob_ and (doc[next_index].ent_iob_ == 'I' or doc[next_index].ent_iob_ == 'U'):\n",
    "                tmp += ' '\n",
    "                tmp += doc[next_index].text\n",
    "                prev_index = next_index\n",
    "                next_index+=1\n",
    "            ner.append(tmp)\n",
    "    print(ner)\n",
    "    docs.append(doc)\n",
    "    sent_vec.update({article:doc.vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
